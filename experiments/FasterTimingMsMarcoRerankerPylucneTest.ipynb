{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterTimingMsMarcoRerankerPylucneTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "Python 3.7.6 64-bit ('venv')",
      "display_name": "Python 3.7.6 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "aea525aded7a940b96fa401bd8481cc3baacffcebb59f934003af67c28cad7f0"
        }
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e7049bf174a4ba19411bc3e7414d354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_857a52d9c06c486cb19fcb8b1afa447b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ebfc0afe53894571b776848e407919a4",
              "IPY_MODEL_820eb1bcea7049c4929bad08747425c2"
            ]
          }
        },
        "857a52d9c06c486cb19fcb8b1afa447b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebfc0afe53894571b776848e407919a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc86f25cb8e140d39c74f2aed740a182",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2637b9c32594e6f958ee73109fc2bb0"
          }
        },
        "820eb1bcea7049c4929bad08747425c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f2f14722cca4c589d00f1bf994b3ced",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/1459 [14:55&lt;40:59:51, 101.72s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28fd721403bb41ef9b0fd7b614bb7ab2"
          }
        },
        "bc86f25cb8e140d39c74f2aed740a182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2637b9c32594e6f958ee73109fc2bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f2f14722cca4c589d00f1bf994b3ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28fd721403bb41ef9b0fd7b614bb7ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e18b5955ae2c4f0f9d03355aba1dfcaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af3013acb3694ae493e4411d0ccf9419",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b13562340e284bf1b52e18f5a39da4dc",
              "IPY_MODEL_e050c8be91f941459cc047c915df85a1"
            ]
          }
        },
        "af3013acb3694ae493e4411d0ccf9419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b13562340e284bf1b52e18f5a39da4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aad71cad425d419ab3c8c2044d11193d",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0aabdbf79d0b48f0837645a7b592f91e"
          }
        },
        "e050c8be91f941459cc047c915df85a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72a445265fa242bb8724408cfbdd1372",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/1459 [01:35&lt;4:16:32, 10.61s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f31aaff511c0499197a078484d2457ef"
          }
        },
        "aad71cad425d419ab3c8c2044d11193d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0aabdbf79d0b48f0837645a7b592f91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72a445265fa242bb8724408cfbdd1372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f31aaff511c0499197a078484d2457ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61314aaafbc74aecb37b7cc246925585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce6f5a348008497192d4b4ca152e276d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ed29e49e8594255b2219482985bf0e4",
              "IPY_MODEL_40ceb8953a8f465f876de05ce5146b82"
            ]
          }
        },
        "ce6f5a348008497192d4b4ca152e276d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ed29e49e8594255b2219482985bf0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1be09a27e507463f97dde556ee39d14c",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcc31d2039ed4bb79a25f977df26cdb4"
          }
        },
        "40ceb8953a8f465f876de05ce5146b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_447c2e27b4774d0c99ef7da6a9f864de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/1459 [07:49&lt;20:49:16, 51.66s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_160f2166503646a3830a6e9935652ab1"
          }
        },
        "1be09a27e507463f97dde556ee39d14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcc31d2039ed4bb79a25f977df26cdb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "447c2e27b4774d0c99ef7da6a9f864de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "160f2166503646a3830a6e9935652ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5797947e1cbb40d08b8a6bd951f1c82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bc4b4e04d1e44ea38fe145ebe23ec5f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_36ef2771c3514b9bb0f37985e4bb6a67",
              "IPY_MODEL_6f7af50f4a7d4c30ac11e478a54cdef1"
            ]
          }
        },
        "bc4b4e04d1e44ea38fe145ebe23ec5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36ef2771c3514b9bb0f37985e4bb6a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_605e0719acd841aaaa40cfe4908b68a0",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87e0782da1854dcbadf6f2e948a428d2"
          }
        },
        "6f7af50f4a7d4c30ac11e478a54cdef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a5fbfa0701d4a0c8d0a475ce1ed79e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/1459 [00:47&lt;2:09:29,  5.35s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_460db5670393411981a0c917205b3de8"
          }
        },
        "605e0719acd841aaaa40cfe4908b68a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87e0782da1854dcbadf6f2e948a428d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a5fbfa0701d4a0c8d0a475ce1ed79e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "460db5670393411981a0c917205b3de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjD05SYZ2xTB",
        "outputId": "dfb99ec9-7a02-4af4-8e81-2180c1a94fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "tags": []
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install transformers==2.10.0 tokenizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct  7 05:21:09 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
            "| N/A   52C    P0    29W /  70W |   4427MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      3147      C   python                           1823MiB |\n",
            "|    0   N/A  N/A      8143      C   /home/ubuntu/venv/bin/python     2601MiB |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already satisfied: transformers==2.10.0 in /home/ubuntu/venv/lib/python3.7/site-packages (2.10.0)\n",
            "Requirement already satisfied: tokenizers in /home/ubuntu/venv/lib/python3.7/site-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (2020.9.27)\n",
            "Requirement already satisfied: requests in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (2.24.0)\n",
            "Requirement already satisfied: sentencepiece in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (0.1.91)\n",
            "Requirement already satisfied: numpy in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (1.19.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (4.50.0)\n",
            "Requirement already satisfied: sacremoses in /home/ubuntu/venv/lib/python3.7/site-packages (from transformers==2.10.0) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests->transformers==2.10.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests->transformers==2.10.0) (1.25.10)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests->transformers==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests->transformers==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: six in /home/ubuntu/venv/lib/python3.7/site-packages (from sacremoses->transformers==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /home/ubuntu/venv/lib/python3.7/site-packages (from sacremoses->transformers==2.10.0) (0.17.0)\n",
            "Requirement already satisfied: click in /home/ubuntu/venv/lib/python3.7/site-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQIDwSSjl9Zf",
        "tags": []
      },
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers import PreTrainedModel\n",
        "from typing import List, Mapping, Tuple, Union, Iterable, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "import abc\n",
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "TokenizerReturnType = Mapping[str, Union[torch.Tensor, List[int],\n",
        "                                         List[List[int]],\n",
        "                                         List[List[str]]]]\n",
        "class Query:\n",
        "    \"\"\"Class representing a query.\n",
        "    A query contains the query text itself and potentially other metadata.\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The query text.\n",
        "    id : Optional[str]\n",
        "        The query id.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str, id: Optional[str] = None):\n",
        "        self.text = text\n",
        "        self.id = id\n",
        "\n",
        "\n",
        "class Text:\n",
        "    \"\"\"Class representing a text to be reranked.\n",
        "    A text is unspecified with respect to it length; in principle, it\n",
        "    could be a full-length document, a paragraph-sized passage, or\n",
        "    even a short phrase.\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to be reranked.\n",
        "    metadata : Mapping[str, Any]\n",
        "        Additional metadata and other annotations.\n",
        "    score : Optional[float]\n",
        "        The score of the text. For example, the score might be the BM25 score\n",
        "        from an initial retrieval stage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 text: str,\n",
        "                 metadata: Mapping[str, Any] = None,\n",
        "                 score: Optional[float] = 0):\n",
        "        self.text = text\n",
        "        if metadata is None:\n",
        "            metadata = dict()\n",
        "        self.metadata = metadata\n",
        "        self.score = score\n",
        "\n",
        "@dataclass\n",
        "class QueryDocumentBatch:\n",
        "    query: Query\n",
        "    documents: List[Text]\n",
        "    output: Optional[TokenizerReturnType] = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "class TokenizerEncodeMixin:\n",
        "    tokenizer: PreTrainedTokenizer = None\n",
        "    tokenizer_kwargs = None\n",
        "\n",
        "    def encode(self, strings: List[str]) -> TokenizerReturnType:\n",
        "        assert self.tokenizer and self.tokenizer_kwargs is not None, \\\n",
        "                'mixin used improperly'\n",
        "        ret = self.tokenizer.batch_encode_plus(strings,\n",
        "                                               **self.tokenizer_kwargs)\n",
        "        ret['tokens'] = list(map(self.tokenizer.tokenize, strings))\n",
        "        return ret\n",
        "\n",
        "class AppendEosTokenizerMixin:\n",
        "    tokenizer: PreTrainedTokenizer = None\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def encode(self, strings: List[str]) -> TokenizerReturnType:\n",
        "        assert self.tokenizer, 'mixin used improperly'\n",
        "        return super().encode(\n",
        "            [f'{x} {self.tokenizer.eos_token}' for x in strings])\n",
        "\n",
        "\n",
        "class QueryDocumentBatchTokenizer(TokenizerEncodeMixin):\n",
        "    def __init__(self,\n",
        "                 tokenizer: PreTrainedTokenizer,\n",
        "                 batch_size: int,\n",
        "                 pattern: str = '{query} {document}',\n",
        "                 **tokenizer_kwargs):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer_kwargs = tokenizer_kwargs\n",
        "        self.pattern = pattern\n",
        "\n",
        "    def traverse_query_document(\n",
        "            self,\n",
        "            batch_input: QueryDocumentBatch) -> Iterable[QueryDocumentBatch]:\n",
        "        query = batch_input.query\n",
        "        for batch_idx in range(0, len(batch_input), self.batch_size):\n",
        "            docs = batch_input.documents[batch_idx:batch_idx + self.batch_size]\n",
        "            outputs = self.encode([self.pattern.format(\n",
        "                                        query=query.text,\n",
        "                                        document=doc.text) for doc in docs])\n",
        "            yield QueryDocumentBatch(query, docs, outputs)\n",
        "\n",
        "class T5BatchTokenizer(AppendEosTokenizerMixin, QueryDocumentBatchTokenizer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs['pattern'] = 'Query: {query} Document: {document} Relevant:'\n",
        "        kwargs['return_attention_mask'] = True\n",
        "        kwargs['padding'] = True\n",
        "        kwargs['return_tensors'] = 'pt'\n",
        "        kwargs['pad_to_max_length'] = True\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "DecodedOutput = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
        "\n",
        "model_inputs = None\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model: PreTrainedModel,\n",
        "                  input_ids: torch.Tensor,\n",
        "                  length: int,\n",
        "                  attention_mask: torch.Tensor = None,\n",
        "                  return_last_logits: bool = True) -> DecodedOutput:\n",
        "    decode_ids = torch.full((input_ids.size(0), 1),\n",
        "                            model.config.decoder_start_token_id,\n",
        "                            dtype=torch.long).to(input_ids.device)\n",
        "    past = model.get_encoder()(input_ids, attention_mask=attention_mask)\n",
        "    next_token_logits = None\n",
        "    for _ in range(length):\n",
        "        model_inputs = model.prepare_inputs_for_generation(\n",
        "            decode_ids,\n",
        "            past=past,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=True)\n",
        "        with autocast():    \n",
        "            outputs = model(**model_inputs)  # (batch_size, cur_len, vocab_size)\n",
        "        next_token_logits = outputs[0][:, -1, :]  # (batch_size, vocab_size)\n",
        "        decode_ids = torch.cat([decode_ids,\n",
        "                                next_token_logits.max(1)[1].unsqueeze(-1)],\n",
        "                               dim=-1)\n",
        "        past = outputs[1]\n",
        "    if return_last_logits:\n",
        "        return decode_ids, next_token_logits, model_inputs\n",
        "    return decode_ids, model_inputs\n",
        "\n",
        "class Reranker:\n",
        "    \"\"\"Class representing a reranker.\n",
        "    A reranker takes a list texts and returns a list of texts non-destructively\n",
        "    (i.e., does not alter the original input list of texts).\n",
        "    \"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def rerank(self, query: Query, texts: List[Text]) -> List[Text]:\n",
        "        \"\"\"Reranks a list of texts with respect to a query.\n",
        "         Parameters\n",
        "         ----------\n",
        "         query : Query\n",
        "             The query.\n",
        "         texts : List[Text]\n",
        "             The list of texts.\n",
        "         Returns\n",
        "         -------\n",
        "         List[Text]\n",
        "             Reranked list of texts.\n",
        "         \"\"\"\n",
        "        pass\n",
        "\n",
        "class T5Reranker(Reranker):\n",
        "    def __init__(self,\n",
        "                 model: T5ForConditionalGeneration,\n",
        "                 tokenizer: QueryDocumentBatchTokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = next(self.model.parameters(), None).device\n",
        "\n",
        "    def rerank(self, query: Query, texts: List[Text]) -> List[Text]:\n",
        "        texts = deepcopy(texts)\n",
        "        batch_input = QueryDocumentBatch(query=query, documents=texts)\n",
        "\n",
        "        # return batch_input\n",
        "        # \"\"\"\n",
        "        for batch in self.tokenizer.traverse_query_document(batch_input):\n",
        "            input_ids = batch.output['input_ids'].to(self.device)\n",
        "            attn_mask = batch.output['attention_mask'].to(self.device)\n",
        "            _, batch_scores, model_inputs = greedy_decode(self.model,\n",
        "                                            input_ids,\n",
        "                                            length=1,\n",
        "                                            attention_mask=attn_mask,\n",
        "                                            return_last_logits=True)\n",
        "\n",
        "            # 6136 and 1176 are the indexes of the tokens false and true in T5.\n",
        "            batch_scores = batch_scores[:, [6136, 1176]]\n",
        "            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
        "            batch_log_probs = batch_scores[:, 1].tolist()\n",
        "            for doc, score in zip(batch.documents, batch_log_probs):\n",
        "                doc.score = score\n",
        "        return texts, model_inputs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92i8-BIDsAmq"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "model_name = 'castorini/monot5-base-msmarco'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device).eval()\n",
        "\n",
        "tokenizer_name = 't5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "tokenizer = T5BatchTokenizer(tokenizer, batch_size)\n",
        "\n",
        "reranker =  T5Reranker(model, tokenizer)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU8eME2mNVA-",
        "outputId": "6301041a-4c6b-4149-d2e0-a964b249ad88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "tags": []
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct  7 05:21:32 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   52C    P0    29W /  70W |   6250MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      3147      C   python                           1823MiB |\n|    0   N/A  N/A      8143      C   /home/ubuntu/venv/bin/python     2601MiB |\n|    0   N/A  N/A      9421      C   /home/ubuntu/venv/bin/python     1823MiB |\n+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_E8yiaGtcqy",
        "outputId": "6fc6152e-ba87-4713-e30d-b3fc9a70915c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "tags": []
      },
      "source": [
        "query = Query('who proposed the geocentric theory')\n",
        "\n",
        "passages = [['7744105', 'For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.'], ['2593796', 'Copernicus proposed a heliocentric model of the solar system Ã¢\\x80\\x93 a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.he geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.'], ['6217200', 'The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.opernicus proposed a heliocentric model of the solar system Ã¢\\x80\\x93 a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.'], ['3276925', 'Copernicus proposed a heliocentric model of the solar system Ã¢\\x80\\x93 a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢\\x80\\x93 which helped convince Galileo that the Earth was not the center of the universe Ã¢\\x80\\x93 can prove that ancient theory incorrect.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.'], ['6217208', 'Copernicus proposed a heliocentric model of the solar system Ã¢\\x80\\x93 a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢\\x80\\x93 which helped convince Galileo that the Earth was not the center of the universe Ã¢\\x80\\x93 can prove that ancient theory incorrect.opernicus proposed a heliocentric model of the solar system Ã¢\\x80\\x93 a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.'], ['4280557', 'The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.imple tools, such as the telescope Ã¢\\x80\\x93 which helped convince Galileo that the Earth was not the center of the universe Ã¢\\x80\\x93 can prove that ancient theory incorrect. You might want to check out one article on the history of the geocentric model and one regarding the geocentric theory.'], ['264181', 'Nicolaus Copernicus (b. 1473Ã¢\\x80\\x93d. 1543) was the first modern author to propose a heliocentric theory of the universe. From the time that Ptolemy of Alexandria (c. 150 CE) constructed a mathematically competent version of geocentric astronomy to CopernicusÃ¢\\x80\\x99s mature heliocentric version (1543), experts knew that the Ptolemaic system diverged from the geocentric concentric-sphere conception of Aristotle.'], ['4280558', 'A Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth. Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth.'], ['3276926', 'The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.'], ['5183032', \"After 1,400 years, Copernicus was the first to propose a theory which differed from Ptolemy's geocentric system, according to which the earth is at rest in the center with the rest of the planets revolving around it.\"]]\n",
        "\n",
        "texts = [ Text(p[1], {'docid': p[0]}, 0) for p in passages] # Note, pyserini scores don't matter since T5 will ignore them.\n",
        "\n",
        "# Either option, let's print out the passages prior to reranking:\n",
        "for i in range(0, 10):\n",
        "    print(f'{i+1:2} {texts[i].metadata[\"docid\"]:15} {texts[i].score:.5f} {texts[i].text}')\n",
        "\n",
        "print('*'*80)\n",
        "\n",
        "# Finally, rerank:\n",
        "reranked, model_inputs = reranker.rerank(query, texts)\n",
        "# temp = reranker.rerank(query, texts)\n",
        "\n",
        "reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "# Print out reranked results:\n",
        "for i in range(0, 10):\n",
        "    print(f'{i+1:2} {reranked[i].metadata[\"docid\"]:15} {reranked[i].score:.5f} {reranked[i].text}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 7744105         0.00000 For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.\n 2 2593796         0.00000 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.he geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.\n 3 6217200         0.00000 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.opernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.\n 4 3276925         0.00000 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.\n 5 6217208         0.00000 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect.opernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.\n 6 4280557         0.00000 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.imple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect. You might want to check out one article on the history of the geocentric model and one regarding the geocentric theory.\n 7 264181          0.00000 Nicolaus Copernicus (b. 1473Ã¢Â€Â“d. 1543) was the first modern author to propose a heliocentric theory of the universe. From the time that Ptolemy of Alexandria (c. 150 CE) constructed a mathematically competent version of geocentric astronomy to CopernicusÃ¢Â€Â™s mature heliocentric version (1543), experts knew that the Ptolemaic system diverged from the geocentric concentric-sphere conception of Aristotle.\n 8 4280558         0.00000 A Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth. Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth.\n 9 3276926         0.00000 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.\n10 5183032         0.00000 After 1,400 years, Copernicus was the first to propose a theory which differed from Ptolemy's geocentric system, according to which the earth is at rest in the center with the rest of the planets revolving around it.\n********************************************************************************\n 1 6217200         -0.01113 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.opernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.\n 2 7744105         -0.01203 For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.\n 3 264181          -0.01991 Nicolaus Copernicus (b. 1473Ã¢Â€Â“d. 1543) was the first modern author to propose a heliocentric theory of the universe. From the time that Ptolemy of Alexandria (c. 150 CE) constructed a mathematically competent version of geocentric astronomy to CopernicusÃ¢Â€Â™s mature heliocentric version (1543), experts knew that the Ptolemaic system diverged from the geocentric concentric-sphere conception of Aristotle.\n 4 2593796         -0.02681 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.he geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.\n 5 6217208         -0.03220 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect.opernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.\n 6 3276925         -0.03442 Copernicus proposed a heliocentric model of the solar system Ã¢Â€Â“ a model where everything orbited around the Sun. Today, with advancements in science and technology, the geocentric model seems preposterous.Simple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.\n 7 5183032         -0.03668 After 1,400 years, Copernicus was the first to propose a theory which differed from Ptolemy's geocentric system, according to which the earth is at rest in the center with the rest of the planets revolving around it.\n 8 3276926         -0.03732 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.ou might want to check out one article on the history of the geocentric model and one regarding the geocentric theory. Here are links to two other articles from Universe Today on what the center of the universe is and Galileo one of the advocates of the heliocentric model.\n 9 4280557         -0.03806 The geocentric model, also known as the Ptolemaic system, is a theory that was developed by philosophers in Ancient Greece and was named after the philosopher Claudius Ptolemy who lived circa 90 to 168 A.D. It was developed to explain how the planets, the Sun, and even the stars orbit around the Earth.imple tools, such as the telescope Ã¢Â€Â“ which helped convince Galileo that the Earth was not the center of the universe Ã¢Â€Â“ can prove that ancient theory incorrect. You might want to check out one article on the history of the geocentric model and one regarding the geocentric theory.\n10 4280558         -2.58398 A Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth. Geocentric theory is an astronomical theory which describes the universe as a Geocentric system, i.e., a system which puts the Earth in the center of the universe, and describes other objects from the point of view of the Earth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQutPfOrLvAy",
        "outputId": "f021ebdf-d1c1-4bd2-b793-4eecc88f6cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "tags": []
      },
      "source": [
        "# !pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1koaGlvulPzMSnnsHsNwbKZeUWRZqPaJM\n",
        "!unzip WHO_Lucene_Search_data.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting gdown\n  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /home/ubuntu/venv/lib/python3.7/site-packages (from gdown) (3.0.12)\nRequirement already satisfied: requests[socks] in /home/ubuntu/venv/lib/python3.7/site-packages (from gdown) (2.24.0)\nRequirement already satisfied: tqdm in /home/ubuntu/venv/lib/python3.7/site-packages (from gdown) (4.50.0)\nRequirement already satisfied: six in /home/ubuntu/venv/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests[socks]-&gt;gdown) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests[socks]-&gt;gdown) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests[socks]-&gt;gdown) (1.25.10)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /home/ubuntu/venv/lib/python3.7/site-packages (from requests[socks]-&gt;gdown) (3.0.4)\nCollecting PySocks!=1.5.7,&gt;=1.5.6; extra == &quot;socks&quot;\n  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=08a9f720d4db879416a7bc51f6f004e8f3eab36d74f97609e5ea3b135d0c4606\n  Stored in directory: /home/ubuntu/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\nSuccessfully built gdown\nInstalling collected packages: gdown, PySocks\nSuccessfully installed PySocks-1.7.1 gdown-3.12.2\nDownloading...\nFrom: https://drive.google.com/uc?id=1koaGlvulPzMSnnsHsNwbKZeUWRZqPaJM\nTo: /home/ubuntu/WHO-FAQ-Rerank-Engine/WHO_Lucene_Search_data.zip\n11.2MB [00:00, 25.6MB/s]\nArchive:  WHO_Lucene_Search_data.zip\n   creating: Search_data/\n  inflating: Search_data/pure_search_5_0.757.p  \n   creating: __MACOSX/\n   creating: __MACOSX/Search_data/\n  inflating: __MACOSX/Search_data/._pure_search_5_0.757.p  \n  inflating: Search_data/pure_search_50_0.905.p  \n  inflating: __MACOSX/Search_data/._pure_search_50_0.905.p  \n  inflating: Search_data/pure_search_100_0.944.p  \n  inflating: __MACOSX/Search_data/._pure_search_100_0.944.p  \n  inflating: Search_data/pure_search_10_0.816.p  \n  inflating: __MACOSX/Search_data/._pure_search_10_0.816.p  \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/ubuntu/WHO-FAQ-Rerank-Engine/notebooks'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR1tqdlKvJ5F",
        "outputId": "b5129597-ea74-447e-b511-fe3a26e443c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "tags": []
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import timeit\n",
        "import numpy as np\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.fastest = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "\n",
        "# torch.set_num_threads(1)\n",
        "\n",
        "gpu_times_dict = {}\n",
        "for x in sorted(os.listdir(\"../Search_data/\")):\n",
        "    title = \"../Search_data/\"+x\n",
        "    if title.endswith(\"checkpoints\"):\n",
        "        continue\n",
        "\n",
        "    batch_size = int(x.split(\"_\")[2])\n",
        "\n",
        "    if batch_size != 50:\n",
        "        continue\n",
        "\n",
        "    print(\"batch size : \", batch_size)\n",
        "\n",
        "    model_name = 'castorini/monot5-base-msmarco'\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name).half()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # device = \"cpu\"\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    tokenizer_name = 't5-base'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    tokenizer = T5BatchTokenizer(tokenizer, batch_size)\n",
        "\n",
        "    reranker =  T5Reranker(model, tokenizer)\n",
        "\n",
        "    with open(title,'rb') as f:\n",
        "        rerank_test = pickle.load(f)\n",
        "\n",
        "    accurate = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "    new_data = []\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    new_title = title.replace(\"pure\",\"reranked\").replace(\"Search_data\",\"drive/My Drive\")\n",
        "    print(new_title)\n",
        "\n",
        "    gpu_times = []\n",
        "\n",
        "    for x in rerank_test:\n",
        "        idx += 1\n",
        "        query_string = x[0]\n",
        "        master_question = x[1]\n",
        "        hits = x[2]\n",
        "        \n",
        "\n",
        "        start = timeit.default_timer()\n",
        "        query = Query(query_string)\n",
        "        texts = [Text(x[1],0) for x in hits]\n",
        "\n",
        "        reranked, model_inputs = reranker.rerank(query, texts)\n",
        "        reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "        stop = timeit.default_timer()\n",
        "\n",
        "        gpu_times.append(stop-start)\n",
        "\n",
        "\n",
        "        scoreDocs = [[x.score, x.text] for x in reranked]\n",
        "\n",
        "        new_data.append([query_string, master_question, scoreDocs])\n",
        "\n",
        "        if idx%30==0:\n",
        "            print(\"mean time : \", np.mean(gpu_times))\n",
        "\n",
        "        if idx%90== 0:\n",
        "            for i in range(0, 5):\n",
        "              print(f'{i+1:2} {reranked[i].score}')\n",
        "            print('$'*80)\n",
        "            break\n",
        "    \n",
        "    gpu_times_dict[new_title]=gpu_times"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size :  50\n",
            "../drive/My Drive/reranked_search_50_0.905.p\n",
            "mean time :  0.1117578198662765\n",
            "mean time :  0.10760342334991341\n",
            "mean time :  0.10647107318889337\n",
            " 1 nan\n",
            " 2 nan\n",
            " 3 nan\n",
            " 4 nan\n",
            " 5 nan\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiZbz8ia67rb",
        "outputId": "1dc156e2-0f66-4cc1-9ca1-d4b3ae9d53de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "1e7049bf174a4ba19411bc3e7414d354",
            "857a52d9c06c486cb19fcb8b1afa447b",
            "ebfc0afe53894571b776848e407919a4",
            "820eb1bcea7049c4929bad08747425c2",
            "bc86f25cb8e140d39c74f2aed740a182",
            "e2637b9c32594e6f958ee73109fc2bb0",
            "7f2f14722cca4c589d00f1bf994b3ced",
            "28fd721403bb41ef9b0fd7b614bb7ab2",
            "e18b5955ae2c4f0f9d03355aba1dfcaa",
            "af3013acb3694ae493e4411d0ccf9419",
            "b13562340e284bf1b52e18f5a39da4dc",
            "e050c8be91f941459cc047c915df85a1",
            "aad71cad425d419ab3c8c2044d11193d",
            "0aabdbf79d0b48f0837645a7b592f91e",
            "72a445265fa242bb8724408cfbdd1372",
            "f31aaff511c0499197a078484d2457ef",
            "61314aaafbc74aecb37b7cc246925585",
            "ce6f5a348008497192d4b4ca152e276d",
            "7ed29e49e8594255b2219482985bf0e4",
            "40ceb8953a8f465f876de05ce5146b82",
            "1be09a27e507463f97dde556ee39d14c",
            "bcc31d2039ed4bb79a25f977df26cdb4",
            "447c2e27b4774d0c99ef7da6a9f864de",
            "160f2166503646a3830a6e9935652ab1",
            "5797947e1cbb40d08b8a6bd951f1c82c",
            "bc4b4e04d1e44ea38fe145ebe23ec5f2",
            "36ef2771c3514b9bb0f37985e4bb6a67",
            "6f7af50f4a7d4c30ac11e478a54cdef1",
            "605e0719acd841aaaa40cfe4908b68a0",
            "87e0782da1854dcbadf6f2e948a428d2",
            "9a5fbfa0701d4a0c8d0a475ce1ed79e4",
            "460db5670393411981a0c917205b3de8"
          ]
        }
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import timeit\n",
        "import numpy as np\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "\n",
        "gpu_times_dict = {}\n",
        "for x in sorted(os.listdir(\"./Search_data/\")):\n",
        "    title = \"./Search_data/\"+x\n",
        "    if title.endswith(\"checkpoints\"):\n",
        "        continue\n",
        "\n",
        "    batch_size = int(x.split(\"_\")[2])\n",
        "    print(\"batch size : \", batch_size)\n",
        "\n",
        "    if batch_size != 50:\n",
        "        continue\n",
        "\n",
        "    model_name = 'castorini/monot5-base-msmarco'\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name).half()\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    tokenizer_name = 't5-base'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    tokenizer = T5BatchTokenizer(tokenizer, batch_size)\n",
        "\n",
        "    reranker =  T5Reranker(model, tokenizer)\n",
        "\n",
        "    with open(title,'rb') as f:\n",
        "        rerank_test = pickle.load(f)\n",
        "\n",
        "    accurate = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "    new_data = []\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    new_title = title.replace(\"pure\",\"reranked\").replace(\"Search_data\",\"drive/My Drive\")\n",
        "    print(new_title)\n",
        "\n",
        "    gpu_times = []\n",
        "\n",
        "    for x in tqdm(rerank_test):\n",
        "        idx += 1\n",
        "        query_string = x[0]\n",
        "        master_question = x[1]\n",
        "        hits = x[2]\n",
        "        \n",
        "\n",
        "        start = timeit.default_timer()\n",
        "        query = Query(query_string)\n",
        "        texts = [Text(x[1],0) for x in hits]\n",
        "\n",
        "        reranked, model_inputs = reranker.rerank(query, texts)\n",
        "        reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "        stop = timeit.default_timer()\n",
        "\n",
        "        gpu_times.append(stop-start)\n",
        "\n",
        "        scoreDocs = [[x.score, x.text] for x in reranked]\n",
        "\n",
        "        new_data.append([query_string, master_question, scoreDocs])\n",
        "\n",
        "        if idx%3==0:\n",
        "            print(\"mean time : \", np.mean(gpu_times))\n",
        "\n",
        "        if idx%9 == 0:\n",
        "            break\n",
        "    \n",
        "    gpu_times_dict[new_title]=gpu_times"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size :  100\n",
            "./drive/My Drive/reranked_search_100_0.944.p\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e7049bf174a4ba19411bc3e7414d354",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "91.68682245899981\n",
            "97.33580770366673\n",
            "99.44855729900013\n",
            "batch size :  10\n",
            "./drive/My Drive/reranked_search_10_0.816.p\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e18b5955ae2c4f0f9d03355aba1dfcaa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "10.668691088333313\n",
            "10.628665557166338\n",
            "10.601329289999638\n",
            "batch size :  50\n",
            "./drive/My Drive/reranked_search_50_0.905.p\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61314aaafbc74aecb37b7cc246925585",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "52.466175864333\n",
            "52.16656846316679\n",
            "52.11167050466651\n",
            "batch size :  5\n",
            "./drive/My Drive/reranked_search_5_0.757.p\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5797947e1cbb40d08b8a6bd951f1c82c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5.307465900332924\n",
            "5.340817393499795\n",
            "5.317168304777523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzV5tHM0GnbR",
        "outputId": "27aa3294-dde0-4e7b-8dec-4aed5ed52ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Oct  5 10:12:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W /  70W |   7377MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmZATDwP9h3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}